# -*- coding: utf-8 -*-
"""muhrakhaal-pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pDU5HQCIoTlH622KWu7XwmaGSr4Dg9s_

# **Proyek Pengembangan Machine Learning Pipeline :**
- **Nama:** Muhammad Rakha Almasah
- **Email:** muh.rakha.al@gmail.com
- **ID Dicoding:** muhrakhaal

# **Tahap 1: Pengolahan Data dalam Machine Learning Pipeline**
"""

!pip install tfx tensorflow-transform --quiet

import os
from google.colab import files
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform
from tfx.proto import example_gen_pb2
import tensorflow_transform as tft

DATA_ROOT = '/content/data'
USERNAME_DICODING = "muhrakhaal"
PIPELINE_ROOT = f'/content/{USERNAME_DICODING}-pipeline'
os.makedirs(DATA_ROOT, exist_ok=True)
os.makedirs(PIPELINE_ROOT, exist_ok=True)

print("Silakan unggah file dataset Anda (format CSV):")
uploaded = files.upload()

for filename in uploaded.keys():
    uploaded_file_path = os.path.join(DATA_ROOT, filename)
    with open(uploaded_file_path, 'wb') as f:
        f.write(uploaded[filename])
    print(f"File berhasil diunggah ke {uploaded_file_path}")

DATA_FILE = uploaded_file_path

context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

output_config = example_gen_pb2.Output(
    split_config=example_gen_pb2.SplitConfig(
        splits=[
            example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
            example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2),
        ]
    )
)
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output_config)
context.run(example_gen)

statistics_gen = StatisticsGen(examples=example_gen.outputs["examples"])
context.run(statistics_gen)

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"])
context.run(schema_gen)

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs["statistics"],
    schema=schema_gen.outputs["schema"]
)
context.run(example_validator)

TRANSFORM_MODULE_FILE = "nasdaq_transform.py"
with open(TRANSFORM_MODULE_FILE, 'w') as f:
    f.write('''
import tensorflow as tf
import tensorflow_transform as tft

LABEL_KEY = "IXIC"
FEATURE_KEYS = ["AAPL", "MSFT", "AMZN", "BRK_B"]

def preprocessing_fn(inputs):
    outputs = {}
    for key in FEATURE_KEYS:
        outputs[key] = tft.scale_to_z_score(inputs[key])
    outputs[LABEL_KEY] = inputs[LABEL_KEY]

    return outputs
''')

transform = Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)
context.run(transform)

context.show(statistics_gen.outputs["statistics"])

context.show(schema_gen.outputs["schema"])

context.show(example_validator.outputs["anomalies"])

"""# **Tahap 2: Pengembangan dan Validasi Model Machine Learning**"""

!pip install keras-tuner --quiet

import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras import layers
from tfx.components import Trainer, Evaluator, Tuner, Pusher
from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.proto import trainer_pb2
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing
from tfx.proto import pusher_pb2
import tensorflow_model_analysis as tfma

FEATURE_KEYS = ["AAPL", "MSFT", "AMZN", "BRK_B"]
LABEL_KEY = "IXIC"

TRAINER_MODULE_FILE = "trainer_module.py"
TUNER_MODULE_FILE = "tuner_module.py"

with open(TUNER_MODULE_FILE, 'w') as f:
    f.write('''
import tensorflow as tf
import keras_tuner as kt
from tensorflow_transform.tf_metadata import schema_utils
from tfx.components.trainer.fn_args_utils import FnArgs
from tensorflow_transform import TFTransformOutput

FEATURE_KEYS = ["AAPL", "MSFT", "AMZN", "BRK_B"]
LABEL_KEY = "IXIC"

def input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=32):
    file_pattern = tf.io.gfile.glob(file_pattern)
    feature_spec = tf_transform_output.raw_feature_spec()

    def parse_fn(serialized_example):
        parsed_features = tf.io.parse_single_example(serialized_example, feature_spec)
        features = {key: parsed_features[key] for key in FEATURE_KEYS}
        label = parsed_features[LABEL_KEY]
        return features, label

    dataset = tf.data.TFRecordDataset(file_pattern, compression_type="GZIP")
    dataset = dataset.map(parse_fn)
    dataset = dataset.shuffle(buffer_size=1000).repeat(num_epochs).batch(batch_size)
    return dataset

class TunerFnResult:
    def __init__(self, tuner, fit_kwargs):
        self.tuner = tuner
        self.fit_kwargs = fit_kwargs

def tuner_fn(fn_args: FnArgs):
    tf_transform_output = TFTransformOutput(fn_args.transform_graph_path)

    def build_model(hp):
        inputs = {key: tf.keras.Input(shape=(1,), name=key) for key in FEATURE_KEYS}
        concatenated = tf.keras.layers.Concatenate()(list(inputs.values()))
        x = tf.keras.layers.Dense(
            units=hp.Int("units_1", min_value=64, max_value=256, step=64),
            activation='relu'
        )(concatenated)
        x = tf.keras.layers.Dense(
            units=hp.Int("units_2", min_value=32, max_value=128, step=32),
            activation='relu'
        )(x)
        outputs = tf.keras.layers.Dense(1)(x)
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        return model

    tuner = kt.RandomSearch(
        hypermodel=build_model,
        objective='val_mae',
        max_trials=50,
        directory=fn_args.working_dir,
        project_name="nasdaq_hyperband"
    )

    train_dataset = input_fn(fn_args.train_files, tf_transform_output)
    validation_dataset = input_fn(fn_args.eval_files, tf_transform_output)

    return TunerFnResult(
        tuner=tuner,
        fit_kwargs={
            "x": train_dataset,
            "validation_data": validation_dataset,
            "steps_per_epoch": fn_args.train_steps,
            "validation_steps": fn_args.eval_steps
        }
    )
''')

tuner = Tuner(
    module_file=os.path.abspath(TUNER_MODULE_FILE),
    examples=transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(num_steps=100),
    eval_args=trainer_pb2.EvalArgs(num_steps=20),
)
context.run(tuner)

with open(TRAINER_MODULE_FILE, 'w') as f:
    f.write('''
import os
import tensorflow as tf
from tensorflow_transform import TFTransformOutput
from tfx.components.trainer.fn_args_utils import FnArgs
import json

FEATURE_KEYS = ["AAPL", "MSFT", "AMZN", "BRK_B"]
LABEL_KEY = "IXIC"

def input_fn(file_pattern, tf_transform_output, num_epochs=None, batch_size=32):
    file_pattern = tf.io.gfile.glob(file_pattern)
    feature_spec = tf_transform_output.raw_feature_spec()

    def parse_fn(serialized_example):
        parsed_features = tf.io.parse_single_example(serialized_example, feature_spec)
        features = {key: tf.cast(parsed_features[key], tf.float32) for key in FEATURE_KEYS}
        label = tf.cast(parsed_features[LABEL_KEY], tf.float32)
        return features, label

    dataset = tf.data.TFRecordDataset(file_pattern, compression_type="GZIP")
    dataset = dataset.map(parse_fn)
    dataset = dataset.shuffle(buffer_size=1000).repeat(num_epochs).batch(batch_size)
    return dataset

def build_model(hparams):
    inputs = {
        key: tf.keras.Input(shape=(1,), name=key)
        for key in FEATURE_KEYS
    }
    concatenated = tf.keras.layers.Concatenate()(list(inputs.values()))

    x = tf.keras.layers.Dense(hparams["units_1"], activation='relu')(concatenated)
    x = tf.keras.layers.Dense(hparams["units_2"], activation='relu')(x)
    outputs = tf.keras.layers.Dense(1)(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

def load_best_hyperparameters(tuner_directory):
    hparams_file = os.path.join(tuner_directory, "best_hyperparameters.txt")
    if not os.path.exists(hparams_file):
        raise FileNotFoundError(f"Hyperparameter tuning file not found at {hparams_file}")

    with open(hparams_file, "r") as f:
        hparams_data = json.load(f)

    hparams = hparams_data["values"]
    return hparams

def run_fn(fn_args):
    tf_transform_output = TFTransformOutput(fn_args.transform_graph_path)

    train_dataset = input_fn(fn_args.train_files, tf_transform_output, num_epochs=fn_args.train_steps)
    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, num_epochs=1)

    tuner_uri = fn_args.custom_config["tuner_hyperparameters_path"]
    hparams = load_best_hyperparameters(tuner_uri)
    model = build_model(hparams)

    model.fit(
        train_dataset,
        validation_data=eval_dataset,
        steps_per_epoch=fn_args.train_steps,
        validation_steps=fn_args.eval_steps,
        epochs=100
    )

    os.makedirs(fn_args.serving_model_dir, exist_ok=True)

    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')])
    def serve_tf_examples_fn(serialized_examples):
        raw_feature_spec = tf_transform_output.raw_feature_spec()
        parsed_features = tf.io.parse_example(serialized_examples, raw_feature_spec)

        model_inputs = {key: parsed_features[key] for key in FEATURE_KEYS}
        return model(model_inputs)

    tf.saved_model.save(
        model,
        fn_args.serving_model_dir,
        signatures={'serving_default': serve_tf_examples_fn}
    )
''')

trainer = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples=transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(num_steps=100),
    eval_args=trainer_pb2.EvalArgs(num_steps=20),
    custom_config={
        "tuner_hyperparameters_path": tuner.outputs["best_hyperparameters"].get()[0].uri
    },
)
context.run(trainer)

resolver = Resolver(
    strategy_class=LatestBlessedModelStrategy,
    model=Channel(type=Model),
    model_blessing=Channel(type=ModelBlessing)
).with_id('latest_blessed_model_resolver')
context.run(resolver)

metrics_specs = [
    tfma.MetricsSpec(
        metrics=[
            tfma.MetricConfig(
                class_name='MeanSquaredError',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(lower_bound={'value': 500.0})
                )
            ),
            tfma.MetricConfig(
                class_name='MeanAbsoluteError',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(lower_bound={'value': 50.0})
                )
            )
        ]
    )
]

evaluation_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key=LABEL_KEY)],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=metrics_specs
)

evaluator = Evaluator(
    examples=transform.outputs['transformed_examples'],
    model=trainer.outputs['model'],
    baseline_model=resolver.outputs['model'],
    eval_config=evaluation_config
)

context.run(evaluator)

pusher = Pusher(
    model=trainer.outputs['model'],
    model_blessing=evaluator.outputs['blessing'],
    push_destination=pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory=f'{PIPELINE_ROOT}/serving_model_dir'
        )
    )
)
context.run(pusher)

context.show(evaluator.outputs['evaluation'])

"""# **Tahap 3: Penerapan Model Machine Learning dalam Sistem Produksi**"""

MODEL_DIR = "/content/muhrakhaal-pipeline/serving_model_dir/1734242302"
loaded_model = tf.saved_model.load(MODEL_DIR)
features = {
    "AAPL": tf.train.Feature(float_list=tf.train.FloatList(value=[176.28])),
    "MSFT": tf.train.Feature(float_list=tf.train.FloatList(value=[336.32])),
    "AMZN": tf.train.Feature(float_list=tf.train.FloatList(value=[138.12])),
    "BRK_B": tf.train.Feature(float_list=tf.train.FloatList(value=[553.50])),
    "IXIC": tf.train.Feature(float_list=tf.train.FloatList(value=[0.0]))  # IXIC sebagai placeholder
}
example = tf.train.Example(features=tf.train.Features(feature=features))
serialized_example = example.SerializeToString()
input_data = {
    'examples': tf.constant([serialized_example])
}
inference_func = loaded_model.signatures['serving_default']
predictions = inference_func(**input_data)

print("Hasil Prediksi:")
print(predictions['output_0'].numpy())

!pip install flask flask-ngrok pyngrok tensorflow --quiet --ignore-installed --no-deps

from flask import Flask, request, jsonify
from pyngrok import ngrok
import tensorflow as tf

app = Flask(__name__)

!ngrok config add-authtoken 2qEftem6N95uNREWujMFvWdpPgB_5L4Yebd9cmvYhRxyvNyeP

MODEL_DIR = "/content/muhrakhaal-pipeline/serving_model_dir/1734242302"
loaded_model = tf.saved_model.load(MODEL_DIR)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        input_data = request.get_json()

        features = input_data.get('features', {})
        tf_features = {
            key: tf.train.Feature(float_list=tf.train.FloatList(value=value))
            for key, value in features.items()
        }
        example = tf.train.Example(features=tf.train.Features(feature=tf_features))
        serialized_example = example.SerializeToString()

        input_tensor = {'examples': tf.constant([serialized_example])}

        inference_func = loaded_model.signatures['serving_default']
        predictions = inference_func(**input_tensor)
        result = predictions['output_0'].numpy().tolist()

        return jsonify({'predictions': result})

    except Exception as e:
        return jsonify({'error': str(e)})

public_url = ngrok.connect(5000)
print(f"Ngrok URL Publik: {public_url}")

if __name__ == '__main__':
    app.run(port=5000)

"""# **Download Pipeline**"""

import shutil
from google.colab import files

FOLDER_PATH = "/content/muhrakhaal-pipeline"

ZIP_FILE_PATH = "/content/muhrakhaal-pipeline.zip"

shutil.make_archive("/content/muhrakhaal-pipeline", 'zip', FOLDER_PATH)

files.download(ZIP_FILE_PATH)